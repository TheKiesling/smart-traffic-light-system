# ğŸš¦ Sistema de SemÃ¡foros Inteligentes con MAPPO

Sistema de control de semÃ¡foros inteligentes basado en **Multi-Agent Proximal Policy Optimization (MAPPO)** con integraciÃ³n completa de **SUMO** (Simulation of Urban MObility). DiseÃ±ado para entrenar mÃºltiples agentes que controlan semÃ¡foros de forma coordinada, optimizando el flujo de trÃ¡fico urbano mediante aprendizaje por refuerzo profundo con soporte GPU.

## ğŸ¯ CaracterÃ­sticas

- âœ… **Algoritmo MAPPO**: ImplementaciÃ³n con Ray RLlib optimizada para multi-agente
- âœ… **IntegraciÃ³n SUMO**: Usando la librerÃ­a `sumo-rl` oficialmente soportada
- âœ… **Soporte GPU**: Entrenamiento acelerado con PyTorch y CUDA
- âœ… **Arquitectura Modular**: CÃ³digo limpio y fÃ¡cilmente extensible
- âœ… **ConfiguraciÃ³n YAML**: ConfiguraciÃ³n flexible sin modificar cÃ³digo
- âœ… **VisualizaciÃ³n**: IntegraciÃ³n con SUMO-GUI para visualizar el comportamiento aprendido
- âœ… **Logging Avanzado**: Sistema automÃ¡tico que guarda mÃ©tricas por iteraciÃ³n (CSV, JSON, grÃ¡ficas)
- âœ… **Restricciones de Tiempo**: Respeto obligatorio de min_green y max_green con `fixed_ts`

## ğŸ“‹ Requisitos Previos

### 1. SUMO (Simulation of Urban MObility)

**Windows:**
```bash
# Descargar e instalar desde:
# https://eclipse.dev/sumo/

# Configurar variable de entorno SUMO_HOME
# Ejemplo: C:\Program Files (x86)\Eclipse\Sumo
```

**Linux/macOS:**
```bash
# Ubuntu/Debian
sudo apt-get install sumo sumo-tools sumo-doc

# macOS (Homebrew)
brew install sumo

# Configurar SUMO_HOME
export SUMO_HOME="/usr/share/sumo"
```

### 2. Python 3.8+

### 3. CUDA (para soporte GPU)
- NVIDIA GPU con soporte CUDA
- CUDA Toolkit 11.8 o superior
- cuDNN compatible

## ğŸš€ InstalaciÃ³n

### 1. Clonar el repositorio

```bash
git clone <tu-repositorio>
cd smart-light-system
```

### 2. Crear entorno virtual

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/macOS
python -m venv venv
source venv/bin/activate
```

### 3. Instalar dependencias

```bash
pip install -r requirements.txt
```

### 4. Verificar instalaciÃ³n de GPU (opcional)

```python
import torch
print(f"GPU disponible: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0)}")
```

## ğŸ“ Estructura del Proyecto

```
smart-light-system/
â”œâ”€â”€ config/                     # Archivos de configuraciÃ³n
â”‚   â”œâ”€â”€ sumo_config.yaml       # ConfiguraciÃ³n de SUMO
â”‚   â””â”€â”€ training_config.yaml   # ConfiguraciÃ³n de entrenamiento MAPPO
â”œâ”€â”€ environments/              # Entornos de RL
â”‚   â””â”€â”€ traffic_env.py        # Environment con sumo-rl
â”œâ”€â”€ models/                    # Modelos y entrenadores
â”‚   â””â”€â”€ mappo_trainer.py      # Entrenador MAPPO con Ray RLlib
â”œâ”€â”€ scenarios/                 # Escenarios de trÃ¡fico SUMO
â”‚   â””â”€â”€ simple_grid/          # IntersecciÃ³n simple
â”‚       â”œâ”€â”€ grid.net.xml      # Red de calles
â”‚       â”œâ”€â”€ grid.rou.xml      # Rutas de vehÃ­culos
â”‚       â””â”€â”€ grid.sumocfg      # ConfiguraciÃ³n SUMO
â”œâ”€â”€ utils/                     # Utilidades
â”‚   â””â”€â”€ helpers.py            # Funciones auxiliares
â”œâ”€â”€ results/                   # Resultados de entrenamiento
â”œâ”€â”€ checkpoints/               # Modelos entrenados
â”œâ”€â”€ logs/                      # Logs de TensorBoard
â”œâ”€â”€ main.py                    # Script principal
â””â”€â”€ requirements.txt           # Dependencias
```

## ğŸ® Uso

### Entrenamiento

```bash
# Entrenamiento bÃ¡sico
python main.py train

# Especificar nÃºmero de iteraciones
python main.py train --iterations 500

# Con generaciÃ³n de grÃ¡ficas
python main.py train --iterations 100 --plot

# ConfiguraciÃ³n personalizada
python main.py train --sumo-config config/sumo_config.yaml --training-config config/training_config.yaml
```

### EvaluaciÃ³n

```bash
# Evaluar modelo entrenado
python main.py evaluate --checkpoint checkpoints/checkpoint_000100

# Evaluar con mÃ¡s episodios
python main.py evaluate --checkpoint checkpoints/checkpoint_000100 --episodes 20
```

### VisualizaciÃ³n

```bash
# Visualizar con SUMO-GUI
python main.py visualize --checkpoint checkpoints/checkpoint_000100

# Personalizar duraciÃ³n
python main.py visualize --checkpoint checkpoints/checkpoint_000100 --duration 1800
```

### AnÃ¡lisis de MÃ©tricas

```bash
# Analizar un entrenamiento especÃ­fico
python scripts/analyze_training.py logs/MAPPO_20251101_143000

# Los logs se generan automÃ¡ticamente durante el entrenamiento en:
# logs/MAPPO_YYYYMMDD_HHMMSS/
#   â”œâ”€â”€ metrics.csv          # MÃ©tricas en formato CSV
#   â”œâ”€â”€ metrics.json         # MÃ©tricas en formato JSON
#   â”œâ”€â”€ summary.txt          # Resumen estadÃ­stico
#   â””â”€â”€ training_metrics.png # GrÃ¡ficas de progreso
```

**MÃ©tricas guardadas por iteraciÃ³n:**
- ğŸ“Š Recompensa media, mÃ¡xima y mÃ­nima
- â±ï¸ Longitud de episodios
- ğŸ“‰ Policy loss y Value function loss
- ğŸ² EntropÃ­a de la polÃ­tica
- ğŸ“ Learning rate actual

Ver documentaciÃ³n completa: [`docs/LOGGING_SYSTEM.md`](docs/LOGGING_SYSTEM.md)

## âš™ï¸ ConfiguraciÃ³n

### SUMO Configuration (`config/sumo_config.yaml`)

```yaml
sumo:
  net_file: "scenarios/simple_grid/grid.net.xml"
  route_file: "scenarios/simple_grid/grid.rou.xml"
  use_gui: false              # true para visualizaciÃ³n
  num_seconds: 3600           # DuraciÃ³n de la simulaciÃ³n
  delta_time: 5               # Segundos entre decisiones
  yellow_time: 2              # DuraciÃ³n de luz amarilla
  min_green: 5                # Tiempo mÃ­nimo en verde
  max_green: 60               # Tiempo mÃ¡ximo en verde
  reward_fn: "diff-waiting-time"  # FunciÃ³n de recompensa
  single_agent: false         # Multi-agente
  sumo_seed: 42               # Semilla para reproducibilidad
```

### Training Configuration (`config/training_config.yaml`)

```yaml
training:
  algorithm: "MAPPO"
  num_workers: 4              # Workers paralelos
  num_gpus: 1                 # GPUs a usar
  framework: "torch"
  
  train_batch_size: 4000
  sgd_minibatch_size: 128
  num_sgd_iter: 10
  
  lr: 0.0003                  # Learning rate
  gamma: 0.99                 # Factor de descuento
  lambda: 0.95                # GAE lambda
  clip_param: 0.2             # PPO clip
  
  model:
    fcnet_hiddens: [256, 256] # Capas ocultas
    fcnet_activation: "relu"
```

## ğŸ“Š Monitoreo

### TensorBoard

```bash
tensorboard --logdir results/
```

Accede a `http://localhost:6006` para ver:
- Recompensa por episodio
- Longitud de episodio
- Policy loss
- Value function loss
- Y mÃ¡s mÃ©tricas

### GrÃ¡ficas automÃ¡ticas

El flag `--plot` genera automÃ¡ticamente grÃ¡ficas de entrenamiento en `results/training_metrics.png`

## ğŸ”§ PersonalizaciÃ³n

### Crear nuevos escenarios

1. DiseÃ±a tu red con **NETEDIT** (incluido con SUMO)
2. Genera rutas con **SUMO tools**
3. Coloca archivos en `scenarios/tu_escenario/`
4. Actualiza `config/sumo_config.yaml`

### Modificar arquitectura del modelo

Edita `config/training_config.yaml`:

```yaml
model:
  fcnet_hiddens: [512, 512, 256]  # Red mÃ¡s profunda
  fcnet_activation: "tanh"        # Cambiar activaciÃ³n
  use_lstm: true                  # Agregar LSTM
  lstm_cell_size: 256
```

### FunciÃ³n de recompensa personalizada

Opciones disponibles en `sumo-rl`:
- `diff-waiting-time`: Diferencia en tiempo de espera
- `average-speed`: Velocidad promedio
- `queue`: Longitud de cola
- `pressure`: PresiÃ³n de trÃ¡fico

## ğŸ“ Algoritmo MAPPO

**MAPPO (Multi-Agent PPO)** es una extensiÃ³n del algoritmo PPO para entornos multi-agente:

- **Centralizado durante entrenamiento**: Usa informaciÃ³n global para aprender mejor
- **Descentralizado durante ejecuciÃ³n**: Cada semÃ¡foro decide independientemente
- **Value function factorization**: Cada agente tiene su propia funciÃ³n de valor
- **Shared or individual policies**: PolÃ­ticas compartidas o individuales

### Ventajas para semÃ¡foros:

âœ… CoordinaciÃ³n implÃ­cita entre semÃ¡foros  
âœ… Estable y sample-efficient  
âœ… Escala bien a mÃºltiples agentes  
âœ… Soporte GPU para entrenamiento rÃ¡pido  

## ğŸ“ˆ Resultados Esperados

DespuÃ©s del entrenamiento, deberÃ­as observar:

- â¬‡ï¸ ReducciÃ³n del tiempo de espera promedio
- â¬†ï¸ Aumento de la velocidad promedio de vehÃ­culos
- â¬‡ï¸ DisminuciÃ³n de longitud de colas
- â¬†ï¸ Mejora en throughput de intersecciones

## ğŸ› SoluciÃ³n de Problemas

### Error: "SUMO_HOME no configurado"

```bash
# Windows (PowerShell)
$env:SUMO_HOME = "C:\Program Files (x86)\Eclipse\Sumo"

# Linux/macOS
export SUMO_HOME="/usr/share/sumo"
```

### Error: "CUDA out of memory"

Reduce en `config/training_config.yaml`:
```yaml
train_batch_size: 2000        # Reducir batch size
num_workers: 2                # Reducir workers
model:
  fcnet_hiddens: [128, 128]   # Red mÃ¡s pequeÃ±a
```

### Entrenamiento lento

Ajusta:
```yaml
num_workers: 8                # MÃ¡s workers si tienes CPUs
rollout_fragment_length: 100  # Fragmentos mÃ¡s cortos
num_gpus: 1                   # Asegurar que usa GPU
```

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una branch (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -m 'Agregar nueva funcionalidad'`)
4. Push a la branch (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

## ğŸ“š Referencias

- **SUMO**: https://eclipse.dev/sumo/
- **sumo-rl**: https://github.com/LucasAlegre/sumo-rl
- **Ray RLlib**: https://docs.ray.io/en/latest/rllib/
- **MAPPO Paper**: https://arxiv.org/abs/2103.01955

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT.

## âœ¨ CrÃ©ditos

Desarrollado con:
- Ray RLlib para MAPPO
- sumo-rl para integraciÃ³n con SUMO
- PyTorch para redes neuronales
- SUMO para simulaciÃ³n de trÃ¡fico

---

**Â¡Feliz entrenamiento! ğŸš¦ğŸ¤–**

Para preguntas o problemas, abre un issue en GitHub.

